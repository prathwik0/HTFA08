{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cocka\\miniconda3\\envs\\AML\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\cocka\\miniconda3\\envs\\AML\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 25)                75        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101 (404.00 Byte)\n",
      "Trainable params: 101 (404.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the discriminator model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# define the discriminator model\n",
    "model = define_discriminator()\n",
    "# summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cocka\\miniconda3\\envs\\AML\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\cocka\\miniconda3\\envs\\AML\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "0 0.59375 0.515625\n",
      "1 0.453125 0.59375\n",
      "2 0.421875 0.46875\n",
      "3 0.578125 0.4375\n",
      "4 0.578125 0.671875\n",
      "5 0.484375 0.53125\n",
      "6 0.4375 0.546875\n",
      "7 0.515625 0.46875\n",
      "8 0.53125 0.625\n",
      "9 0.578125 0.53125\n",
      "10 0.53125 0.4375\n",
      "11 0.546875 0.59375\n",
      "12 0.40625 0.4375\n",
      "13 0.46875 0.5625\n",
      "14 0.59375 0.75\n",
      "15 0.609375 0.640625\n",
      "16 0.4375 0.53125\n",
      "17 0.59375 0.484375\n",
      "18 0.515625 0.453125\n",
      "19 0.484375 0.53125\n",
      "20 0.53125 0.578125\n",
      "21 0.453125 0.640625\n",
      "22 0.40625 0.5625\n",
      "23 0.53125 0.546875\n",
      "24 0.609375 0.578125\n",
      "25 0.53125 0.5625\n",
      "26 0.453125 0.671875\n",
      "27 0.625 0.671875\n",
      "28 0.5 0.578125\n",
      "29 0.515625 0.515625\n",
      "30 0.515625 0.515625\n",
      "31 0.484375 0.59375\n",
      "32 0.578125 0.546875\n",
      "33 0.546875 0.640625\n",
      "34 0.578125 0.65625\n",
      "35 0.53125 0.65625\n",
      "36 0.53125 0.65625\n",
      "37 0.59375 0.640625\n",
      "38 0.5 0.59375\n",
      "39 0.46875 0.75\n",
      "40 0.546875 0.6875\n",
      "41 0.609375 0.71875\n",
      "42 0.5 0.703125\n",
      "43 0.453125 0.890625\n",
      "44 0.625 0.703125\n",
      "45 0.46875 0.71875\n",
      "46 0.53125 0.734375\n",
      "47 0.484375 0.78125\n",
      "48 0.578125 0.75\n",
      "49 0.578125 0.765625\n",
      "50 0.671875 0.828125\n",
      "51 0.453125 0.796875\n",
      "52 0.609375 0.859375\n",
      "53 0.53125 0.890625\n",
      "54 0.546875 0.921875\n",
      "55 0.453125 0.890625\n",
      "56 0.484375 0.859375\n",
      "57 0.59375 0.921875\n",
      "58 0.65625 0.859375\n",
      "59 0.5625 0.875\n",
      "60 0.609375 0.890625\n",
      "61 0.5625 0.953125\n",
      "62 0.5625 0.875\n",
      "63 0.4375 0.890625\n",
      "64 0.65625 0.828125\n",
      "65 0.421875 0.84375\n",
      "66 0.578125 0.953125\n",
      "67 0.5625 0.921875\n",
      "68 0.5 0.90625\n",
      "69 0.5 0.96875\n",
      "70 0.515625 0.953125\n",
      "71 0.5 0.953125\n",
      "72 0.546875 0.9375\n",
      "73 0.46875 0.875\n",
      "74 0.578125 0.953125\n",
      "75 0.484375 0.9375\n",
      "76 0.390625 0.953125\n",
      "77 0.4375 0.9375\n",
      "78 0.53125 0.9375\n",
      "79 0.515625 0.984375\n",
      "80 0.53125 0.984375\n",
      "81 0.46875 0.984375\n",
      "82 0.578125 0.984375\n",
      "83 0.421875 0.984375\n",
      "84 0.46875 0.984375\n",
      "85 0.40625 0.953125\n",
      "86 0.421875 0.96875\n",
      "87 0.453125 0.96875\n",
      "88 0.375 1.0\n",
      "89 0.5 0.96875\n",
      "90 0.40625 0.984375\n",
      "91 0.5 0.96875\n",
      "92 0.453125 1.0\n",
      "93 0.40625 0.953125\n",
      "94 0.453125 0.921875\n",
      "95 0.34375 0.96875\n",
      "96 0.421875 0.96875\n",
      "97 0.34375 0.96875\n",
      "98 0.4375 0.953125\n",
      "99 0.40625 0.984375\n",
      "100 0.40625 0.9375\n",
      "101 0.25 1.0\n",
      "102 0.421875 0.96875\n",
      "103 0.53125 0.984375\n",
      "104 0.34375 1.0\n",
      "105 0.421875 0.984375\n",
      "106 0.40625 0.96875\n",
      "107 0.40625 0.9375\n",
      "108 0.515625 1.0\n",
      "109 0.515625 0.984375\n",
      "110 0.296875 1.0\n",
      "111 0.5 0.96875\n",
      "112 0.484375 0.96875\n",
      "113 0.4375 0.9375\n",
      "114 0.453125 1.0\n",
      "115 0.484375 0.984375\n",
      "116 0.484375 0.984375\n",
      "117 0.484375 0.96875\n",
      "118 0.453125 0.984375\n",
      "119 0.390625 0.984375\n",
      "120 0.515625 0.96875\n",
      "121 0.4375 0.9375\n",
      "122 0.4375 0.984375\n",
      "123 0.453125 0.984375\n",
      "124 0.40625 1.0\n",
      "125 0.484375 0.953125\n",
      "126 0.328125 0.96875\n",
      "127 0.375 0.96875\n",
      "128 0.484375 0.984375\n",
      "129 0.46875 0.953125\n",
      "130 0.453125 0.984375\n",
      "131 0.484375 1.0\n",
      "132 0.328125 0.96875\n",
      "133 0.5625 0.953125\n",
      "134 0.359375 0.96875\n",
      "135 0.390625 0.953125\n",
      "136 0.453125 0.984375\n",
      "137 0.5 0.96875\n",
      "138 0.5 0.953125\n",
      "139 0.453125 0.984375\n",
      "140 0.46875 0.9375\n",
      "141 0.484375 0.953125\n",
      "142 0.375 1.0\n",
      "143 0.4375 0.984375\n",
      "144 0.359375 0.953125\n",
      "145 0.453125 0.9375\n",
      "146 0.46875 0.9375\n",
      "147 0.453125 0.984375\n",
      "148 0.421875 0.953125\n",
      "149 0.375 0.984375\n",
      "150 0.4375 0.96875\n",
      "151 0.59375 0.953125\n",
      "152 0.453125 0.984375\n",
      "153 0.46875 0.984375\n",
      "154 0.5 0.953125\n",
      "155 0.4375 0.96875\n",
      "156 0.453125 0.96875\n",
      "157 0.421875 0.921875\n",
      "158 0.515625 0.96875\n",
      "159 0.46875 1.0\n",
      "160 0.46875 0.984375\n",
      "161 0.59375 0.96875\n",
      "162 0.515625 0.96875\n",
      "163 0.484375 0.96875\n",
      "164 0.484375 1.0\n",
      "165 0.5 0.984375\n",
      "166 0.40625 0.953125\n",
      "167 0.515625 0.953125\n",
      "168 0.546875 0.9375\n",
      "169 0.5 0.984375\n",
      "170 0.53125 0.96875\n",
      "171 0.46875 0.984375\n",
      "172 0.484375 0.9375\n",
      "173 0.609375 0.984375\n",
      "174 0.40625 0.9375\n",
      "175 0.53125 0.984375\n",
      "176 0.359375 0.9375\n",
      "177 0.453125 0.953125\n",
      "178 0.53125 0.953125\n",
      "179 0.5625 0.921875\n",
      "180 0.546875 0.953125\n",
      "181 0.53125 0.96875\n",
      "182 0.5625 1.0\n",
      "183 0.375 0.953125\n",
      "184 0.59375 0.96875\n",
      "185 0.5 0.984375\n",
      "186 0.515625 0.984375\n",
      "187 0.578125 1.0\n",
      "188 0.546875 0.953125\n",
      "189 0.5 0.953125\n",
      "190 0.640625 0.953125\n",
      "191 0.5625 0.921875\n",
      "192 0.421875 0.984375\n",
      "193 0.546875 0.96875\n",
      "194 0.59375 0.96875\n",
      "195 0.5625 0.96875\n",
      "196 0.421875 0.953125\n",
      "197 0.5 0.96875\n",
      "198 0.53125 0.953125\n",
      "199 0.546875 1.0\n",
      "200 0.59375 0.90625\n",
      "201 0.53125 0.984375\n",
      "202 0.5 0.9375\n",
      "203 0.578125 0.859375\n",
      "204 0.609375 0.9375\n",
      "205 0.53125 0.96875\n",
      "206 0.5625 0.96875\n",
      "207 0.515625 0.96875\n",
      "208 0.546875 0.921875\n",
      "209 0.625 0.96875\n",
      "210 0.609375 0.96875\n",
      "211 0.640625 0.984375\n",
      "212 0.5625 0.9375\n",
      "213 0.734375 0.984375\n",
      "214 0.609375 0.96875\n",
      "215 0.65625 0.953125\n",
      "216 0.640625 0.953125\n",
      "217 0.609375 0.953125\n",
      "218 0.609375 0.984375\n",
      "219 0.625 0.96875\n",
      "220 0.5 0.96875\n",
      "221 0.734375 0.984375\n",
      "222 0.515625 0.90625\n",
      "223 0.5625 0.953125\n",
      "224 0.453125 0.9375\n",
      "225 0.71875 0.9375\n",
      "226 0.734375 1.0\n",
      "227 0.734375 0.984375\n",
      "228 0.625 0.96875\n",
      "229 0.671875 0.921875\n",
      "230 0.609375 0.890625\n",
      "231 0.65625 0.96875\n",
      "232 0.640625 0.96875\n",
      "233 0.59375 0.953125\n",
      "234 0.59375 0.96875\n",
      "235 0.625 0.96875\n",
      "236 0.640625 0.953125\n",
      "237 0.6875 0.953125\n",
      "238 0.78125 0.96875\n",
      "239 0.671875 0.9375\n",
      "240 0.6875 0.984375\n",
      "241 0.65625 0.9375\n",
      "242 0.609375 0.9375\n",
      "243 0.671875 0.9375\n",
      "244 0.625 0.953125\n",
      "245 0.65625 0.953125\n",
      "246 0.71875 0.875\n",
      "247 0.734375 0.96875\n",
      "248 0.78125 0.96875\n",
      "249 0.75 0.921875\n",
      "250 0.71875 0.96875\n",
      "251 0.734375 0.9375\n",
      "252 0.625 0.9375\n",
      "253 0.6875 0.953125\n",
      "254 0.796875 0.921875\n",
      "255 0.71875 0.953125\n",
      "256 0.734375 0.9375\n",
      "257 0.78125 0.96875\n",
      "258 0.765625 0.90625\n",
      "259 0.71875 0.984375\n",
      "260 0.71875 0.84375\n",
      "261 0.734375 0.953125\n",
      "262 0.703125 0.90625\n",
      "263 0.796875 0.921875\n",
      "264 0.78125 0.890625\n",
      "265 0.78125 0.96875\n",
      "266 0.671875 0.921875\n",
      "267 0.765625 0.953125\n",
      "268 0.640625 0.9375\n",
      "269 0.765625 0.921875\n",
      "270 0.59375 0.90625\n",
      "271 0.765625 0.921875\n",
      "272 0.65625 0.96875\n",
      "273 0.71875 0.90625\n",
      "274 0.75 0.890625\n",
      "275 0.703125 0.921875\n",
      "276 0.625 0.875\n",
      "277 0.78125 0.953125\n",
      "278 0.703125 0.96875\n",
      "279 0.78125 0.875\n",
      "280 0.703125 0.921875\n",
      "281 0.828125 0.921875\n",
      "282 0.8125 0.9375\n",
      "283 0.84375 0.890625\n",
      "284 0.6875 0.96875\n",
      "285 0.765625 0.90625\n",
      "286 0.71875 0.90625\n",
      "287 0.78125 0.921875\n",
      "288 0.796875 0.921875\n",
      "289 0.75 0.84375\n",
      "290 0.78125 0.9375\n",
      "291 0.828125 0.9375\n",
      "292 0.84375 0.96875\n",
      "293 0.859375 0.96875\n",
      "294 0.8125 0.90625\n",
      "295 0.78125 0.9375\n",
      "296 0.796875 0.921875\n",
      "297 0.84375 0.890625\n",
      "298 0.703125 0.9375\n",
      "299 0.9375 0.9375\n",
      "300 0.734375 0.953125\n",
      "301 0.78125 0.921875\n",
      "302 0.875 0.9375\n",
      "303 0.921875 0.90625\n",
      "304 0.75 0.875\n",
      "305 0.859375 0.921875\n",
      "306 0.78125 0.953125\n",
      "307 0.8125 0.90625\n",
      "308 0.828125 0.96875\n",
      "309 0.875 0.921875\n",
      "310 0.890625 0.96875\n",
      "311 0.828125 1.0\n",
      "312 0.765625 0.921875\n",
      "313 0.8125 0.953125\n",
      "314 0.859375 0.984375\n",
      "315 0.78125 0.890625\n",
      "316 0.84375 0.921875\n",
      "317 0.828125 0.921875\n",
      "318 0.8125 0.859375\n",
      "319 0.8125 0.9375\n",
      "320 0.859375 0.96875\n",
      "321 0.875 0.921875\n",
      "322 0.84375 0.9375\n",
      "323 0.796875 0.921875\n",
      "324 0.75 0.953125\n",
      "325 0.84375 0.921875\n",
      "326 0.875 0.953125\n",
      "327 0.796875 0.9375\n",
      "328 0.8125 0.9375\n",
      "329 0.859375 0.90625\n",
      "330 0.921875 0.90625\n",
      "331 0.890625 0.921875\n",
      "332 0.84375 0.890625\n",
      "333 0.90625 0.9375\n",
      "334 0.8125 0.90625\n",
      "335 0.84375 0.890625\n",
      "336 0.875 0.921875\n",
      "337 0.84375 0.96875\n",
      "338 0.859375 0.984375\n",
      "339 0.875 0.90625\n",
      "340 0.9375 0.921875\n",
      "341 0.84375 0.9375\n",
      "342 0.859375 0.921875\n",
      "343 0.921875 0.984375\n",
      "344 0.8125 0.890625\n",
      "345 0.9375 0.9375\n",
      "346 0.953125 0.984375\n",
      "347 0.84375 0.953125\n",
      "348 0.890625 0.84375\n",
      "349 0.921875 0.9375\n",
      "350 0.90625 0.90625\n",
      "351 0.9375 0.90625\n",
      "352 0.859375 0.875\n",
      "353 0.890625 0.890625\n",
      "354 0.796875 0.90625\n",
      "355 0.953125 0.921875\n",
      "356 0.84375 0.921875\n",
      "357 0.890625 0.90625\n",
      "358 0.8125 0.90625\n",
      "359 0.890625 0.9375\n",
      "360 0.875 0.890625\n",
      "361 0.875 0.90625\n",
      "362 0.90625 0.9375\n",
      "363 0.9375 0.859375\n",
      "364 0.9375 0.859375\n",
      "365 0.90625 0.890625\n",
      "366 0.953125 0.890625\n",
      "367 0.765625 0.9375\n",
      "368 0.875 0.9375\n",
      "369 0.859375 0.953125\n",
      "370 0.890625 0.984375\n",
      "371 0.921875 0.921875\n",
      "372 0.859375 0.953125\n",
      "373 0.921875 0.9375\n",
      "374 0.890625 0.921875\n",
      "375 0.84375 0.9375\n",
      "376 0.890625 0.859375\n",
      "377 0.90625 0.9375\n",
      "378 0.984375 0.875\n",
      "379 0.90625 0.84375\n",
      "380 0.921875 0.921875\n",
      "381 0.90625 0.921875\n",
      "382 0.921875 0.90625\n",
      "383 0.90625 0.84375\n",
      "384 0.859375 0.921875\n",
      "385 0.921875 0.96875\n",
      "386 0.890625 0.921875\n",
      "387 0.953125 0.859375\n",
      "388 0.984375 0.921875\n",
      "389 0.9375 0.953125\n",
      "390 0.953125 0.875\n",
      "391 0.90625 0.921875\n",
      "392 0.9375 0.890625\n",
      "393 0.921875 0.875\n",
      "394 0.921875 0.828125\n",
      "395 0.875 0.921875\n",
      "396 0.953125 0.921875\n",
      "397 0.90625 0.921875\n",
      "398 0.96875 0.890625\n",
      "399 0.984375 0.90625\n",
      "400 0.9375 0.8125\n",
      "401 0.921875 0.953125\n",
      "402 0.921875 0.953125\n",
      "403 0.984375 0.921875\n",
      "404 0.953125 0.90625\n",
      "405 0.921875 0.875\n",
      "406 0.9375 0.921875\n",
      "407 0.9375 0.828125\n",
      "408 0.984375 0.90625\n",
      "409 1.0 0.90625\n",
      "410 0.953125 0.859375\n",
      "411 0.953125 0.921875\n",
      "412 0.953125 0.90625\n",
      "413 1.0 0.890625\n",
      "414 0.96875 0.921875\n",
      "415 0.984375 0.890625\n",
      "416 0.953125 0.859375\n",
      "417 1.0 0.90625\n",
      "418 0.96875 0.9375\n",
      "419 0.953125 0.984375\n",
      "420 0.96875 0.875\n",
      "421 0.96875 0.90625\n",
      "422 0.96875 0.90625\n",
      "423 0.96875 0.890625\n",
      "424 1.0 0.890625\n",
      "425 0.9375 0.90625\n",
      "426 0.96875 0.96875\n",
      "427 0.921875 0.890625\n",
      "428 0.984375 0.96875\n",
      "429 0.984375 0.90625\n",
      "430 0.9375 0.984375\n",
      "431 0.96875 0.921875\n",
      "432 0.984375 0.90625\n",
      "433 0.953125 0.859375\n",
      "434 0.984375 0.921875\n",
      "435 0.921875 0.875\n",
      "436 0.90625 0.859375\n",
      "437 0.9375 0.90625\n",
      "438 0.96875 0.90625\n",
      "439 0.953125 0.875\n",
      "440 0.953125 0.859375\n",
      "441 0.953125 0.859375\n",
      "442 0.984375 0.90625\n",
      "443 0.9375 0.875\n",
      "444 0.984375 0.859375\n",
      "445 0.9375 0.90625\n",
      "446 1.0 0.8125\n",
      "447 0.984375 0.890625\n",
      "448 0.96875 0.875\n",
      "449 0.96875 0.90625\n",
      "450 0.953125 0.859375\n",
      "451 0.953125 0.84375\n",
      "452 0.984375 0.84375\n",
      "453 0.953125 0.90625\n",
      "454 1.0 0.9375\n",
      "455 0.984375 0.875\n",
      "456 0.984375 0.875\n",
      "457 0.953125 0.859375\n",
      "458 0.96875 0.90625\n",
      "459 1.0 0.875\n",
      "460 0.984375 0.9375\n",
      "461 0.984375 0.921875\n",
      "462 0.96875 0.875\n",
      "463 0.953125 0.859375\n",
      "464 0.984375 0.875\n",
      "465 1.0 0.875\n",
      "466 0.984375 0.859375\n",
      "467 0.984375 0.890625\n",
      "468 0.984375 0.890625\n",
      "469 1.0 0.90625\n",
      "470 0.984375 0.921875\n",
      "471 0.984375 0.890625\n",
      "472 0.921875 0.90625\n",
      "473 1.0 0.84375\n",
      "474 0.9375 0.859375\n",
      "475 0.9375 0.90625\n",
      "476 0.984375 0.875\n",
      "477 0.984375 0.921875\n",
      "478 1.0 0.84375\n",
      "479 0.984375 0.875\n",
      "480 0.984375 0.90625\n",
      "481 0.9375 0.90625\n",
      "482 0.953125 0.84375\n",
      "483 0.9375 0.796875\n",
      "484 0.984375 0.90625\n",
      "485 0.984375 0.890625\n",
      "486 0.96875 0.90625\n",
      "487 0.9375 0.921875\n",
      "488 0.96875 0.90625\n",
      "489 0.984375 0.921875\n",
      "490 0.984375 0.890625\n",
      "491 0.9375 0.875\n",
      "492 0.984375 0.875\n",
      "493 0.96875 0.90625\n",
      "494 0.984375 0.9375\n",
      "495 0.984375 0.875\n",
      "496 1.0 0.90625\n",
      "497 1.0 0.828125\n",
      "498 0.9375 0.859375\n",
      "499 0.984375 0.921875\n",
      "500 0.984375 0.90625\n",
      "501 0.984375 0.921875\n",
      "502 1.0 0.890625\n",
      "503 0.96875 0.796875\n",
      "504 1.0 0.984375\n",
      "505 1.0 0.921875\n",
      "506 0.984375 0.84375\n",
      "507 0.984375 0.90625\n",
      "508 1.0 0.875\n",
      "509 0.96875 0.875\n",
      "510 0.984375 0.90625\n",
      "511 1.0 0.875\n",
      "512 1.0 0.859375\n",
      "513 1.0 0.828125\n",
      "514 1.0 0.84375\n",
      "515 0.984375 0.859375\n",
      "516 0.96875 0.796875\n",
      "517 0.953125 0.859375\n",
      "518 1.0 0.84375\n",
      "519 0.984375 0.890625\n",
      "520 0.984375 0.890625\n",
      "521 0.984375 0.875\n",
      "522 1.0 0.796875\n",
      "523 0.984375 0.875\n",
      "524 0.984375 0.796875\n",
      "525 1.0 0.96875\n",
      "526 0.984375 0.890625\n",
      "527 1.0 0.9375\n",
      "528 0.984375 0.859375\n",
      "529 1.0 0.953125\n",
      "530 0.96875 0.875\n",
      "531 1.0 0.875\n",
      "532 1.0 0.75\n",
      "533 0.984375 0.859375\n",
      "534 0.984375 0.8125\n",
      "535 1.0 0.84375\n",
      "536 1.0 0.90625\n",
      "537 1.0 0.890625\n",
      "538 1.0 0.90625\n",
      "539 0.984375 0.765625\n",
      "540 1.0 0.875\n",
      "541 0.96875 0.796875\n",
      "542 1.0 0.890625\n",
      "543 1.0 0.890625\n",
      "544 1.0 0.78125\n",
      "545 1.0 0.96875\n",
      "546 0.96875 0.9375\n",
      "547 1.0 0.828125\n",
      "548 1.0 0.875\n",
      "549 1.0 0.9375\n",
      "550 1.0 0.921875\n",
      "551 1.0 0.921875\n",
      "552 1.0 0.921875\n",
      "553 1.0 0.859375\n",
      "554 1.0 0.859375\n",
      "555 1.0 0.921875\n",
      "556 1.0 0.828125\n",
      "557 1.0 0.9375\n",
      "558 1.0 0.859375\n",
      "559 1.0 0.875\n",
      "560 1.0 0.890625\n",
      "561 1.0 0.828125\n",
      "562 1.0 0.90625\n",
      "563 1.0 0.9375\n",
      "564 1.0 0.921875\n",
      "565 1.0 0.875\n",
      "566 1.0 0.875\n",
      "567 1.0 0.921875\n",
      "568 1.0 0.953125\n",
      "569 1.0 0.828125\n",
      "570 1.0 0.875\n",
      "571 1.0 0.890625\n",
      "572 0.984375 0.921875\n",
      "573 1.0 0.90625\n",
      "574 1.0 0.90625\n",
      "575 1.0 0.84375\n",
      "576 1.0 0.921875\n",
      "577 1.0 0.875\n",
      "578 1.0 0.90625\n",
      "579 1.0 0.921875\n",
      "580 1.0 0.84375\n",
      "581 1.0 0.8125\n",
      "582 1.0 0.875\n",
      "583 1.0 0.90625\n",
      "584 1.0 0.84375\n",
      "585 1.0 0.890625\n",
      "586 1.0 0.9375\n",
      "587 1.0 0.84375\n",
      "588 1.0 0.875\n",
      "589 1.0 0.859375\n",
      "590 1.0 0.78125\n",
      "591 1.0 0.84375\n",
      "592 1.0 0.90625\n",
      "593 1.0 0.890625\n",
      "594 1.0 0.96875\n",
      "595 1.0 0.8125\n",
      "596 1.0 0.84375\n",
      "597 1.0 0.828125\n",
      "598 1.0 0.875\n",
      "599 1.0 0.859375\n",
      "600 1.0 0.890625\n",
      "601 1.0 0.875\n",
      "602 1.0 0.890625\n",
      "603 1.0 0.765625\n",
      "604 1.0 0.828125\n",
      "605 1.0 0.84375\n",
      "606 1.0 0.890625\n",
      "607 1.0 0.84375\n",
      "608 1.0 0.875\n",
      "609 1.0 0.890625\n",
      "610 1.0 0.875\n",
      "611 1.0 0.921875\n",
      "612 1.0 0.890625\n",
      "613 1.0 0.875\n",
      "614 1.0 0.8125\n",
      "615 1.0 0.875\n",
      "616 1.0 0.828125\n",
      "617 1.0 0.84375\n",
      "618 1.0 0.875\n",
      "619 1.0 0.859375\n",
      "620 1.0 0.859375\n",
      "621 1.0 0.875\n",
      "622 1.0 0.890625\n",
      "623 1.0 0.9375\n",
      "624 1.0 0.90625\n",
      "625 1.0 0.84375\n",
      "626 1.0 0.828125\n",
      "627 1.0 0.875\n",
      "628 1.0 0.90625\n",
      "629 1.0 0.828125\n",
      "630 1.0 0.875\n",
      "631 1.0 0.890625\n",
      "632 1.0 0.84375\n",
      "633 1.0 0.90625\n",
      "634 1.0 0.8125\n",
      "635 1.0 0.875\n",
      "636 1.0 0.921875\n",
      "637 1.0 0.84375\n",
      "638 1.0 0.921875\n",
      "639 1.0 0.828125\n",
      "640 1.0 0.90625\n",
      "641 1.0 0.859375\n",
      "642 1.0 0.890625\n",
      "643 1.0 0.8125\n",
      "644 1.0 0.921875\n",
      "645 1.0 0.84375\n",
      "646 1.0 0.953125\n",
      "647 1.0 0.890625\n",
      "648 1.0 0.8125\n",
      "649 1.0 0.828125\n",
      "650 1.0 0.921875\n",
      "651 1.0 0.953125\n",
      "652 1.0 0.921875\n",
      "653 1.0 0.875\n",
      "654 1.0 0.859375\n",
      "655 1.0 0.890625\n",
      "656 1.0 0.890625\n",
      "657 1.0 0.90625\n",
      "658 1.0 0.796875\n",
      "659 1.0 0.875\n",
      "660 1.0 0.9375\n",
      "661 1.0 0.890625\n",
      "662 1.0 0.78125\n",
      "663 1.0 0.828125\n",
      "664 1.0 0.8125\n",
      "665 1.0 0.90625\n",
      "666 1.0 0.921875\n",
      "667 1.0 0.875\n",
      "668 1.0 0.875\n",
      "669 1.0 0.859375\n",
      "670 1.0 0.90625\n",
      "671 1.0 0.84375\n",
      "672 1.0 0.84375\n",
      "673 1.0 0.921875\n",
      "674 1.0 0.78125\n",
      "675 1.0 0.84375\n",
      "676 1.0 0.875\n",
      "677 1.0 0.828125\n",
      "678 1.0 0.859375\n",
      "679 1.0 0.859375\n",
      "680 1.0 0.90625\n",
      "681 1.0 0.90625\n",
      "682 1.0 0.84375\n",
      "683 1.0 0.859375\n",
      "684 1.0 0.875\n",
      "685 1.0 0.90625\n",
      "686 1.0 0.890625\n",
      "687 1.0 0.84375\n",
      "688 1.0 0.875\n",
      "689 1.0 0.875\n",
      "690 1.0 0.890625\n",
      "691 1.0 0.921875\n",
      "692 1.0 0.890625\n",
      "693 1.0 0.875\n",
      "694 1.0 0.875\n",
      "695 1.0 0.828125\n",
      "696 1.0 0.890625\n",
      "697 1.0 0.859375\n",
      "698 1.0 0.859375\n",
      "699 1.0 0.859375\n",
      "700 1.0 0.953125\n",
      "701 1.0 0.875\n",
      "702 1.0 0.796875\n",
      "703 1.0 0.828125\n",
      "704 1.0 0.9375\n",
      "705 1.0 0.765625\n",
      "706 1.0 0.859375\n",
      "707 1.0 0.859375\n",
      "708 1.0 0.890625\n",
      "709 1.0 0.796875\n",
      "710 1.0 0.78125\n",
      "711 1.0 0.875\n",
      "712 1.0 0.84375\n",
      "713 1.0 0.875\n",
      "714 1.0 0.90625\n",
      "715 1.0 0.890625\n",
      "716 1.0 0.859375\n",
      "717 1.0 0.890625\n",
      "718 1.0 0.765625\n",
      "719 1.0 0.84375\n",
      "720 1.0 0.859375\n",
      "721 1.0 0.921875\n",
      "722 1.0 0.8125\n",
      "723 1.0 0.90625\n",
      "724 1.0 0.9375\n",
      "725 1.0 0.90625\n",
      "726 1.0 0.84375\n",
      "727 1.0 0.90625\n",
      "728 1.0 0.84375\n",
      "729 1.0 0.828125\n",
      "730 1.0 0.765625\n",
      "731 1.0 0.890625\n",
      "732 1.0 0.828125\n",
      "733 1.0 0.859375\n",
      "734 1.0 0.796875\n",
      "735 1.0 0.890625\n",
      "736 1.0 0.875\n",
      "737 1.0 0.859375\n",
      "738 1.0 0.90625\n",
      "739 1.0 0.75\n",
      "740 1.0 0.796875\n",
      "741 1.0 0.90625\n",
      "742 1.0 0.890625\n",
      "743 1.0 0.828125\n",
      "744 1.0 0.828125\n",
      "745 1.0 0.90625\n",
      "746 1.0 0.9375\n",
      "747 1.0 0.875\n",
      "748 1.0 0.890625\n",
      "749 1.0 0.859375\n",
      "750 1.0 0.9375\n",
      "751 1.0 0.8125\n",
      "752 1.0 0.875\n",
      "753 1.0 0.84375\n",
      "754 1.0 0.8125\n",
      "755 1.0 0.875\n",
      "756 1.0 0.84375\n",
      "757 1.0 0.796875\n",
      "758 1.0 0.90625\n",
      "759 1.0 0.890625\n",
      "760 1.0 0.78125\n",
      "761 1.0 0.796875\n",
      "762 1.0 0.8125\n",
      "763 1.0 0.765625\n",
      "764 1.0 0.859375\n",
      "765 1.0 0.921875\n",
      "766 1.0 0.859375\n",
      "767 1.0 0.765625\n",
      "768 1.0 0.921875\n",
      "769 1.0 0.765625\n",
      "770 1.0 0.90625\n",
      "771 1.0 0.859375\n",
      "772 1.0 0.828125\n",
      "773 1.0 0.828125\n",
      "774 1.0 0.9375\n",
      "775 1.0 0.90625\n",
      "776 1.0 0.875\n",
      "777 1.0 0.90625\n",
      "778 1.0 0.796875\n",
      "779 1.0 0.84375\n",
      "780 1.0 0.890625\n",
      "781 1.0 0.828125\n",
      "782 1.0 0.890625\n",
      "783 1.0 0.828125\n",
      "784 1.0 0.8125\n",
      "785 1.0 0.890625\n",
      "786 1.0 0.90625\n",
      "787 1.0 0.9375\n",
      "788 1.0 0.890625\n",
      "789 1.0 0.890625\n",
      "790 1.0 0.90625\n",
      "791 1.0 0.921875\n",
      "792 1.0 0.890625\n",
      "793 1.0 0.859375\n",
      "794 1.0 0.796875\n",
      "795 1.0 0.875\n",
      "796 1.0 0.859375\n",
      "797 1.0 0.875\n",
      "798 1.0 0.796875\n",
      "799 1.0 0.90625\n",
      "800 1.0 0.796875\n",
      "801 1.0 0.828125\n",
      "802 1.0 0.875\n",
      "803 1.0 0.890625\n",
      "804 1.0 0.78125\n",
      "805 1.0 0.84375\n",
      "806 1.0 0.90625\n",
      "807 1.0 0.890625\n",
      "808 1.0 0.78125\n",
      "809 1.0 0.859375\n",
      "810 1.0 0.875\n",
      "811 1.0 0.921875\n",
      "812 1.0 0.921875\n",
      "813 1.0 0.796875\n",
      "814 1.0 0.890625\n",
      "815 1.0 0.921875\n",
      "816 1.0 0.890625\n",
      "817 1.0 0.90625\n",
      "818 1.0 0.875\n",
      "819 1.0 0.859375\n",
      "820 1.0 0.78125\n",
      "821 1.0 0.828125\n",
      "822 1.0 0.890625\n",
      "823 1.0 0.890625\n",
      "824 1.0 0.859375\n",
      "825 1.0 0.875\n",
      "826 1.0 0.859375\n",
      "827 1.0 0.859375\n",
      "828 1.0 0.828125\n",
      "829 1.0 0.890625\n",
      "830 1.0 0.875\n",
      "831 1.0 0.84375\n",
      "832 1.0 0.921875\n",
      "833 1.0 0.765625\n",
      "834 1.0 0.796875\n",
      "835 1.0 0.890625\n",
      "836 1.0 0.875\n",
      "837 1.0 0.890625\n",
      "838 1.0 0.90625\n",
      "839 1.0 0.890625\n",
      "840 1.0 0.921875\n",
      "841 1.0 0.90625\n",
      "842 1.0 0.859375\n",
      "843 1.0 0.8125\n",
      "844 1.0 0.875\n",
      "845 1.0 0.859375\n",
      "846 1.0 0.8125\n",
      "847 1.0 0.875\n",
      "848 1.0 0.90625\n",
      "849 1.0 0.875\n",
      "850 1.0 0.875\n",
      "851 1.0 0.9375\n",
      "852 1.0 0.8125\n",
      "853 1.0 0.828125\n",
      "854 1.0 0.875\n",
      "855 1.0 0.875\n",
      "856 1.0 0.984375\n",
      "857 1.0 0.9375\n",
      "858 1.0 0.890625\n",
      "859 1.0 0.890625\n",
      "860 1.0 0.859375\n",
      "861 1.0 0.84375\n",
      "862 1.0 0.90625\n",
      "863 1.0 0.890625\n",
      "864 1.0 0.890625\n",
      "865 1.0 0.890625\n",
      "866 1.0 0.890625\n",
      "867 1.0 0.859375\n",
      "868 1.0 0.9375\n",
      "869 1.0 0.875\n",
      "870 1.0 0.890625\n",
      "871 1.0 0.859375\n",
      "872 1.0 0.859375\n",
      "873 1.0 0.875\n",
      "874 1.0 0.875\n",
      "875 1.0 0.90625\n",
      "876 1.0 0.796875\n",
      "877 1.0 0.921875\n",
      "878 1.0 0.828125\n",
      "879 1.0 0.78125\n",
      "880 1.0 0.90625\n",
      "881 1.0 0.890625\n",
      "882 1.0 0.859375\n",
      "883 1.0 0.875\n",
      "884 1.0 0.921875\n",
      "885 1.0 0.859375\n",
      "886 1.0 0.890625\n",
      "887 1.0 0.859375\n",
      "888 1.0 0.890625\n",
      "889 1.0 0.859375\n",
      "890 1.0 0.859375\n",
      "891 1.0 0.875\n",
      "892 1.0 0.8125\n",
      "893 1.0 0.890625\n",
      "894 1.0 0.953125\n",
      "895 1.0 0.859375\n",
      "896 1.0 0.796875\n",
      "897 1.0 0.8125\n",
      "898 1.0 0.84375\n",
      "899 1.0 0.84375\n",
      "900 1.0 0.8125\n",
      "901 1.0 0.859375\n",
      "902 1.0 0.84375\n",
      "903 1.0 0.90625\n",
      "904 1.0 0.796875\n",
      "905 1.0 0.875\n",
      "906 1.0 0.84375\n",
      "907 1.0 0.9375\n",
      "908 1.0 0.875\n",
      "909 1.0 0.875\n",
      "910 1.0 0.921875\n",
      "911 1.0 0.8125\n",
      "912 1.0 0.921875\n",
      "913 1.0 0.859375\n",
      "914 1.0 0.890625\n",
      "915 1.0 0.875\n",
      "916 1.0 0.859375\n",
      "917 1.0 0.890625\n",
      "918 1.0 0.8125\n",
      "919 1.0 0.90625\n",
      "920 1.0 0.859375\n",
      "921 1.0 0.859375\n",
      "922 1.0 0.796875\n",
      "923 1.0 0.9375\n",
      "924 1.0 0.9375\n",
      "925 1.0 0.859375\n",
      "926 1.0 0.78125\n",
      "927 1.0 0.90625\n",
      "928 1.0 0.890625\n",
      "929 1.0 0.84375\n",
      "930 1.0 0.8125\n",
      "931 1.0 0.921875\n",
      "932 1.0 0.828125\n",
      "933 1.0 0.921875\n",
      "934 1.0 0.859375\n",
      "935 1.0 0.796875\n",
      "936 1.0 0.890625\n",
      "937 1.0 0.875\n",
      "938 1.0 0.90625\n",
      "939 1.0 0.796875\n",
      "940 1.0 0.84375\n",
      "941 1.0 0.859375\n",
      "942 1.0 0.78125\n",
      "943 1.0 0.921875\n",
      "944 1.0 0.875\n",
      "945 1.0 0.90625\n",
      "946 1.0 0.953125\n",
      "947 1.0 0.90625\n",
      "948 1.0 0.828125\n",
      "949 1.0 0.859375\n",
      "950 1.0 0.859375\n",
      "951 1.0 0.78125\n",
      "952 1.0 0.875\n",
      "953 1.0 0.84375\n",
      "954 1.0 0.90625\n",
      "955 1.0 0.890625\n",
      "956 1.0 0.875\n",
      "957 1.0 0.859375\n",
      "958 1.0 0.90625\n",
      "959 1.0 0.90625\n",
      "960 1.0 0.796875\n",
      "961 1.0 0.890625\n",
      "962 1.0 0.90625\n",
      "963 1.0 0.859375\n",
      "964 1.0 0.859375\n",
      "965 1.0 0.90625\n",
      "966 1.0 0.84375\n",
      "967 1.0 0.921875\n",
      "968 1.0 0.875\n",
      "969 1.0 0.875\n",
      "970 1.0 0.8125\n",
      "971 1.0 0.859375\n",
      "972 1.0 0.875\n",
      "973 1.0 0.8125\n",
      "974 1.0 0.890625\n",
      "975 1.0 0.875\n",
      "976 1.0 0.9375\n",
      "977 1.0 0.859375\n",
      "978 1.0 0.859375\n",
      "979 1.0 0.84375\n",
      "980 1.0 0.859375\n",
      "981 1.0 0.921875\n",
      "982 1.0 0.859375\n",
      "983 1.0 0.859375\n",
      "984 1.0 0.890625\n",
      "985 1.0 0.859375\n",
      "986 1.0 0.90625\n",
      "987 1.0 0.875\n",
      "988 1.0 0.90625\n",
      "989 1.0 0.875\n",
      "990 1.0 0.875\n",
      "991 1.0 0.84375\n",
      "992 1.0 0.875\n",
      "993 1.0 0.96875\n",
      "994 1.0 0.859375\n",
      "995 1.0 0.90625\n",
      "996 1.0 0.859375\n",
      "997 1.0 0.828125\n",
      "998 1.0 0.828125\n",
      "999 1.0 0.921875\n"
     ]
    }
   ],
   "source": [
    "# define and fit a discriminator model\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import hstack\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# generate n real samples with class labels\n",
    "def generate_real_samples(n):\n",
    "\t# generate inputs in [-0.5, 0.5]\n",
    "\tX1 = rand(n) - 0.5\n",
    "\t# generate outputs X^2\n",
    "\tX2 = X1 * X1\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = ones((n, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# generate n fake samples with class labels\n",
    "def generate_fake_samples(n):\n",
    "\t# generate inputs in [-1, 1]\n",
    "\tX1 = -1 + rand(n) * 2\n",
    "\t# generate outputs in [-1, 1]\n",
    "\tX2 = -1 + rand(n) * 2\n",
    "\t# stack arrays\n",
    "\tX1 = X1.reshape(n, 1)\n",
    "\tX2 = X2.reshape(n, 1)\n",
    "\tX = hstack((X1, X2))\n",
    "\t# generate class labels\n",
    "\ty = zeros((n, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# train the discriminator model\n",
    "def train_discriminator(model, n_epochs=1000, n_batch=128):\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# run epochs manually\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# generate real examples\n",
    "\t\tX_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# update model\n",
    "\t\tmodel.train_on_batch(X_real, y_real)\n",
    "\t\t# generate fake examples\n",
    "\t\tX_fake, y_fake = generate_fake_samples(half_batch)\n",
    "\t\t# update model\n",
    "\t\tmodel.train_on_batch(X_fake, y_fake)\n",
    "\t\t# evaluate the model\n",
    "\t\t_, acc_real = model.evaluate(X_real, y_real, verbose=0)\n",
    "\t\t_, acc_fake = model.evaluate(X_fake, y_fake, verbose=0)\n",
    "\t\tprint(i, acc_real, acc_fake)\n",
    "\n",
    "# define the discriminator model\n",
    "model = define_discriminator()\n",
    "# fit the model\n",
    "train_discriminator(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define and use the generator model\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = randn(latent_dim * n)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "# use the generator to generate n fake examples and plot the results\n",
    "def generate_fake_samples(generator, latent_dim, n):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n)\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# plot the results\n",
    "\tpyplot.scatter(X[:, 0], X[:, 1])\n",
    "\tpyplot.show()\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# define the discriminator model\n",
    "model = define_generator(latent_dim)\n",
    "# generate and plot generated samples\n",
    "generate_fake_samples(model, latent_dim, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_4 (Sequential)   (None, 2)                 122       \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 223 (892.00 Byte)\n",
      "Trainable params: 122 (488.00 Byte)\n",
      "Non-trainable params: 101 (404.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# demonstrate creating the three models in the gan\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the standalone discriminator model\n",
    "def define_discriminator(n_inputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(25, activation='relu', kernel_initializer='he_uniform', input_dim=n_inputs))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=2):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(15, activation='relu', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(n_outputs, activation='linear'))\n",
    "\treturn model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "\t# make weights in the discriminator not trainable\n",
    "\tdiscriminator.trainable = False\n",
    "\t# connect them\n",
    "\tmodel = Sequential()\n",
    "\t# add generator\n",
    "\tmodel.add(generator)\n",
    "\t# add the discriminator\n",
    "\tmodel.add(discriminator)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\treturn model\n",
    "\n",
    "# size of the latent space\n",
    "latent_dim = 5\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator()\n",
    "# create the generator\n",
    "generator = define_generator(latent_dim)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "# summarize gan model\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the composite model\n",
    "def train_gan(gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128):\n",
    "\t# determine half the size of one batch, for updating the discriminator\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare real samples\n",
    "\t\tx_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# prepare fake examples\n",
    "\t\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator\n",
    "\t\td_model.train_on_batch(x_real, y_real)\n",
    "\t\td_model.train_on_batch(x_fake, y_fake)\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the discriminator and plot real and fake points\n",
    "def summarize_performance(epoch, generator, discriminator, latent_dim, n=100):\n",
    "\t# prepare real samples\n",
    "\tx_real, y_real = generate_real_samples(n)\n",
    "\t# evaluate discriminator on real examples\n",
    "\t_, acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "\t# prepare fake examples\n",
    "\tx_fake, y_fake = generate_fake_samples(generator, latent_dim, n)\n",
    "\t# evaluate discriminator on fake examples\n",
    "\t_, acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "\t# summarize discriminator performance\n",
    "\tprint(epoch, acc_real, acc_fake)\n",
    "\t# scatter plot real and fake data points\n",
    "\tpyplot.scatter(x_real[:, 0], x_real[:, 1], color='red')\n",
    "\tpyplot.scatter(x_fake[:, 0], x_fake[:, 1], color='blue')\n",
    "\tpyplot.show()\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, latent_dim, n_epochs=10000, n_batch=128, n_eval=2000):\n",
    "\t# determine half the size of one batch, for updating the discriminator\n",
    "\thalf_batch = int(n_batch / 2)\n",
    "\t# manually enumerate epochs\n",
    "\tfor i in range(n_epochs):\n",
    "\t\t# prepare real samples\n",
    "\t\tx_real, y_real = generate_real_samples(half_batch)\n",
    "\t\t# prepare fake examples\n",
    "\t\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\t\t# update discriminator\n",
    "\t\td_model.train_on_batch(x_real, y_real)\n",
    "\t\td_model.train_on_batch(x_fake, y_fake)\n",
    "\t\t# prepare points in latent space as input for the generator\n",
    "\t\tx_gan = generate_latent_points(latent_dim, n_batch)\n",
    "\t\t# create inverted labels for the fake samples\n",
    "\t\ty_gan = ones((n_batch, 1))\n",
    "\t\t# update the generator via the discriminator's error\n",
    "\t\tgan_model.train_on_batch(x_gan, y_gan)\n",
    "\t\t# evaluate the model every n_eval epochs\n",
    "\t\tif (i+1) % n_eval == 0:\n",
    "\t\t\tsummarize_performance(i, g_model, d_model, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
